<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Case Study – COVID-19 Diagnosis with Audio Biomarkers | Arash Nicoomanesh</title>
  <meta name="description" content="COVID-19 diagnosis through acoustic analysis of breathing, cough, and speech signals (illustrative demo results)">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <style>
    /* === DESIGN TOKENS === */
    :root {
      --bg: #ffffff; /* White background */
      --fg: #333333; /* Dark text for white background */
      --accent: #ff6b00;
      --accent-light: #ff9d5c;
      --muted: #f5f5f5; /* Light gray for cards */
      --card-bg: #f9f9f9;
      --max-width: 900px;
      --radius: 12px;
      --font: "Inter", system-ui, sans-serif;
    }

    *, *::before, *::after { box-sizing: border-box; }
    html { scroll-behavior: smooth; }
    body {
      margin: 0;
      font-family: var(--font);
      line-height: 1.7;
      background: var(--bg);
      color: var(--fg);
    }

    /* === LAYOUT SHELL === */
    header {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      padding: 1rem;
      background: rgba(255,255,255,.95);
      backdrop-filter: blur(10px);
      z-index: 1000;
      border-bottom: 1px solid rgba(0,0,0,0.1);
    }
    
    .header-container {
      max-width: var(--max-width);
      margin: 0 auto;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }
    
    header a {
      color: var(--accent);
      text-decoration: none;
      font-weight: 500;
      display: flex;
      align-items: center;
      gap: 0.5rem;
      transition: opacity 0.2s;
    }
    
    header a:hover {
      opacity: 0.8;
    }
    
    .case-nav {
      display: flex;
      gap: 1.5rem;
    }
    
    .case-nav a {
      color: var(--fg);
      font-size: 0.9rem;
    }
    
    .case-nav a:hover {
      color: var(--accent);
    }

    main {
      max-width: var(--max-width);
      margin: 0 auto;
      padding: clamp(5rem, 6vw + 2rem, 7rem) 1.5rem 3rem;
    }

    /* === TYPOGRAPHY === */
    h1 { 
      font-size: clamp(2rem, 4vw, 2.8rem); 
      margin: 0 0 0.5rem; 
      line-height: 1.2;
      color: #333333; /* Dark text for white background */
    }
    
    .subtitle { 
      font-size: 1.1rem; 
      color: #666; 
      margin: 0 0 1rem;
      font-weight: 400;
    }

    .disclaimer {
      font-size: 0.9rem;
      color: #8a6d3b;
      background: #fff4e5;
      border-left: 4px solid #ffcc80;
      padding: 0.8rem 1rem;
      border-radius: 8px;
      margin-bottom: 1.5rem;
    }
    
    h2 {
      font-size: 1.5rem;
      color: #333333; /* Dark text for white background */
      margin: 2.5rem 0 1rem;
      font-weight: 600;
      position: relative;
      padding-bottom: 0.5rem;
    }
    
    h2::after {
      content: '';
      position: absolute;
      bottom: 0;
      left: 0;
      width: 50px;
      height: 3px;
      background: var(--accent);
      border-radius: 3px;
    }
    
    p, li { 
      margin: 0 0 1rem;
      font-weight: 300;
      color: #444; /* Slightly lighter than headings */
    }
    
    ul, ol { 
      padding-left: 1.6rem;
      margin-bottom: 1.5rem;
    }
    
    li { 
      margin-bottom: 0.8rem;
      position: relative;
    }
    
    ul li::before {
      content: '•';
      color: var(--accent);
      font-weight: bold;
      display: inline-block;
      width: 1em;
      margin-left: -1em;
    }
    
    strong {
      color: #222;
      font-weight: 500;
    }

    /* === CARDS === */
    .feature-card {
      background: var(--card-bg);
      border-radius: var(--radius);
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-left: 4px solid var(--accent);
      box-shadow: 0 5px 15px rgba(0,0,0,0.08);
    }
    
    .feature-card h3 {
      margin-top: 0;
      color: #333333; /* Dark text for white background */
      font-size: 1.2rem;
    }

    /* === MEDIA === */
    .hero-img {
      width: 100%;
      height: auto;
      border-radius: var(--radius);
      margin: 2rem 0;
      box-shadow: 0 8px 25px rgba(0,0,0,0.1);
      border: 1px solid #eee;
    }
    
    .img-caption {
      text-align: center;
      font-style: italic;
      color: #666;
      margin-top: -1rem;
      margin-bottom: 2rem;
      font-size: 0.9rem;
    }

    /* === TECH TAGS === */
    .tech-tags {
      display: flex;
      flex-wrap: wrap;
      gap: 0.7rem;
      margin: 2rem 0;
    }
    
    .tech-tags span {
      background: #000000; /* Black background */
      color: #ffffff; /* White text */
      padding: 0.5rem 1rem;
      border-radius: 30px;
      font-size: 0.85rem;
      border: none;
      font-weight: 500;
    }

    /* === DIVIDER === */
    .divider {
      height: 1px;
      background: linear-gradient(90deg, transparent, var(--accent) 30%, var(--accent) 70%, transparent);
      margin: 3rem 0;
    }

    /* === METHODOLOGY STEPS === */
    .methodology-steps {
      counter-reset: step-counter;
    }
    
    .methodology-step {
      margin-bottom: 1.5rem;
      padding-left: 2.5rem;
      position: relative;
    }
    
    .methodology-step:before {
      counter-increment: step-counter;
      content: counter(step-counter);
      position: absolute;
      left: 0;
      top: 0;
      background: #000000;
      color: white;
      width: 28px;
      height: 28px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.9rem;
      font-weight: 600;
    }
    
    .step-content strong {
      color: #000000;
      display: block;
      margin-bottom: 0.4rem;
    }

    /* === BUTTONS === */
    .case-footer {
      display: flex;
      justify-content: center;
      align-items: center;
      margin-top: 4rem;
      padding-top: 2rem;
      border-top: 1px solid rgba(0,0,0,0.1);
      flex-wrap: wrap;
      gap: 1rem;
    }
    
    .case-footer a {
      color: var(--accent); /* Orange text */
      text-decoration: none;
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      padding: 0.7rem 1.5rem;
      background: #000000; /* Black background */
      border-radius: 30px;
      transition: all 0.2s;
      border: none;
    }
    
    .case-footer a:hover {
      background: var(--accent); /* Orange background on hover */
      color: #000000; /* Black text on hover */
      transform: translateY(-2px);
    }

    /* === TABLES === */
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
      margin-bottom: 1rem;
    }
    table th, table td {
      text-align: left;
      padding: 0.6rem 0.8rem;
      border-bottom: 1px solid #eee;
      font-weight: 300;
      color: #444;
    }
    table th { font-weight: 600; color: #222; }

    /* === RESPONSIVE === */
    @media (max-width: 768px) {
      .header-container {
        flex-direction: column;
        gap: 1rem;
        text-align: center;
      }
      
      .case-nav {
        justify-content: center;
      }
      
      .case-footer {
        flex-direction: column;
        text-align: center;
      }
      
      .case-footer a {
        width: 100%;
        justify-content: center;
      }
    }
    
    @media (max-width: 600px) {
      main { 
        padding: clamp(4rem, 8vw, 5.5rem) 1rem 2rem; 
      }
      
      .tech-tags {
        justify-content: center;
      }
      
      .methodology-step {
        padding-left: 2.2rem;
      }
      
      .methodology-step:before {
        width: 24px;
        height: 24px;
        font-size: 0.8rem;
      }
    }

    /* cosmetic adjustments kept from original template */
    .feature-card { border-left: none !important; }
    h2 { display: inline-block; padding-bottom: 0.4rem; max-width: 100%; }
    h2::after { width: 100%; height: 2px; background: var(--accent); }
  </style>
</head>

<body>
  <header>
    <div class="header-container">
      <a href="../index.html#portfolio"><i class="fas fa-arrow-left"></i> Back to Portfolio</a>
      <div class="case-nav">
        <a href="#challenges">Challenges</a>
        <a href="#solution">Solution</a>
        <a href="#methodology">Methodology</a>
        <a href="#results">Results</a>
        <a href="#tech">Tech</a>
      </div>
    </div>
  </header>

  <main>
    <h1>Case Study: COVID-19 Diagnosis with Audio Biomarkers</h1>
    <p class="subtitle">Non-invasive COVID-19 screening using acoustic analysis of cough, breath and short speech recorded on consumer devices.</p>

    <div class="disclaimer">
      <strong>Demo results shown below are illustrative.</strong> The numeric values and charts on this page are scaled. They are **not** validated clinical performance 
    </div>
    <img class="hero-img" src="cov2.jpg" alt="COVID-19">
    <section id="challenges">
      <h2>Challenges</h2>
      <p>The COVID-19 pandemic highlighted a global need for fast, widely accessible screening tools. Acoustic analysis of respiratory sounds offers a low-cost, non-invasive option but raises several technical and ethical challenges.</p>
      
      <div class="feature-card">
        <h3>Key Challenges</h3>
        <ul>
          <li><strong>Data scarcity & labeling:</strong> Curated, clinically-verified audio datasets are limited and heterogeneous.</li>
          <li><strong>Confounders:</strong> Device, environment, language, age, and comorbidity create spurious correlations that can bias models.</li>
          <li><strong>Signal variability:</strong> Breath/cough/speech acoustic signatures differ across people and recording conditions.</li>
          <li><strong>Regulatory sensitivity:</strong> Clinical claims require external validation and ethical safeguards (IRB, consent, privacy).</li>
        </ul>
      </div>
    </section>

    <div class="divider"></div>

    <section id="solution">
      <h2>Solution & Architecture</h2>
      <p>We built a reproducible pipeline that combines careful preprocessing, multiple baseline models (classical + CNN + self-supervised transfer), and a deployment-ready distillation recipe for a compact on-device classifier. The pipeline emphasizes patient-level evaluation, confounder analysis, and explainability.</p>
      
      <img class="hero-img" src="cov1.jpeg" alt="COVID-19">
      <p class="img-caption">Audio pipeline: collection → segmentation → features / SSL embeddings → model training → explainability → distillation + mobile deployment.</p>

      <div class="feature-card">
        <h3>Key Components</h3>
        <ol class="solution-list">
          <li>
            <strong>Audio Data Collection & Metadata</strong>
            <p>Standardized CSV metadata (patient_id, recording_id, age, sex, covid_status, pcr_date, days_since_pcr, device_type, environment, sample_type, file_path). Patient-level identifiers are used for all splits to prevent leakage.</p>
          </li>
          <li>
            <strong>Preprocessing & Segmentation</strong>
            <p>Resampling to 16 kHz, RMS normalization, optional high-pass denoising, and event detection (energy-based VAD or pretrained cough detector) to produce fixed-length segments for modeling.</p>
          </li>
          <li>
            <strong>Feature & Embedding Extraction</strong>
            <p>Two parallel feature tracks: (1) hand-crafted MFCC/spectral/time-domain features aggregated by segment, and (2) learned embeddings from SSL backbones (wav2vec2 / HuBERT / PANNs / AST).</p>
          </li>
          <li>
            <strong>Modeling & Evaluation</strong>
            <p>Benchmarks include classical (MFCC + XGBoost), CNN on log-mel, and fine-tuned SSL models. Primary evaluation uses patient-level holdout and bootstrap CI reporting.</p>
          </li>
          <li>
            <strong>Explainability & Biomarker Validation</strong>
            <p>Feature importance (SHAP) for tabular models and saliency/IG for spectrogram models to validate candidate biomarkers with clinical plausibility.</p>
          </li>
          <li>
            <strong>Productionization</strong>
            <p>Distill the best model into a compact CNN, apply quantization/pruning, and benchmark latency & memory on target mobile devices.</p>
          </li>
        </ol>
      </div>
    </section>

    <section id="methodology">
      <h2>Methodology</h2>
      
      <div class="feature-card">
        <h3>Research workflow</h3>
        <div class="methodology-steps">
          <div class="methodology-step">
            <div class="step-content">
              <strong>1) Dataset construction & curation</strong>
              <p>Collect metadata with PCR-confirmed labels; perform manual QC to remove clipped/low-SNR files; store objective metadata fields and an audit trail for each recording.</p>
            </div>
          </div>
          
          <div class="methodology-step">
            <div class="step-content">
              <strong>2) Preprocessing & segmentation</strong>
              <p>Resample to 16 kHz, normalize amplitude, apply optional bandpass/notch filters, run VAD/cough detector to extract 0.5–3 s cough segments and 1–10 s breath/speech segments.</p>
            </div>
          </div>
          
          <div class="methodology-step">
            <div class="step-content">
              <strong>3) Feature extraction</strong>
              <p>Compute MFCCs (13–40 with deltas), log-mel spectrograms (128 bins), spectral centroids, entropy, jitter/shimmer (for speech), and SSL embeddings (wav2vec2/HuBERT last hidden states aggregated by mean).</p>
            </div>
          </div>
          
          <div class="methodology-step">
            <div class="step-content">
              <strong>4) Modeling & augmentation</strong>
              <p>Train classical and deep models with heavy augmentation (SpecAugment, time-stretch, pitch shift, SNR schedule, Mixup/CutMix). Use patient-level splits and early stopping on validation AUROC.</p>
            </div>
          </div>
          
          <div class="methodology-step">
            <div class="step-content">
              <strong>5) Evaluation & confounder analysis</strong>
              <p>Report AUROC, sensitivity/specificity at target operating points, F1, and 95% bootstrap CIs. Produce subgroup breakdowns (age, sex, device-type) and test whether metadata alone predicts label.</p>
            </div>
          </div>
          
          <div class="methodology-step">
            <div class="step-content">
              <strong>6) Explainability & biomarker validation</strong>
              <p>Apply SHAP and integrated gradients to identify important frequency bands/time regions; consult clinical literature to align acoustic findings with known respiratory physiology.</p>
            </div>
          </div>
        </div>
      </div>

      <div class="feature-card">
        <h3>Audio Modalities & Tasks</h3>
        <ul>
          <li><strong>Breathing:</strong> forced and normal breathing segments — analyze respiratory cadence and wheeze-like signatures</li>
          <li><strong>Cough:</strong> voluntary coughs segmented by onset energy and analyzed for spectral bursts and temporal envelope</li>
          <li><strong>Speech:</strong> sustained vowels and short read sentences — analyze voice quality, shimmer/jitter, and spectral tilt</li>
          <li><strong>Multimodal fusion:</strong> late-fusion ensemble combining per-modality classifiers to maximize robustness</li>
        </ul>
      </div>
    </section>

    <section>
      <h2>Technical Approach</h2>
      
      <div class="feature-card">
        <h3>Model families evaluated</h3>
        <p>We benchmarked the following model families. In each case we report per-segment and per-patient aggregated results (patient-level majority vote).</p>

        <table>
          <thead>
            <tr><th>Family</th><th>Input</th><th>Notes</th></tr>
          </thead>
          <tbody>
            <tr><td><strong>Classical (XGBoost)</strong></td><td>MFCC + spectral + time-domain aggregated stats</td><td>Robust baseline for low-data regimes; SHAP for feature importance</td></tr>
            <tr><td><strong>CNN (log-mel)</strong></td><td>Log-mel spectrogram (128 bins)</td><td>Lightweight architecture, SpecAugment, good candidate for distillation</td></tr>
            <tr><td><strong>SSL transfer</strong></td><td>Raw waveform → wav2vec2 / HuBERT embeddings</td><td>Freeze + linear probe → fine-tune last layers; best transfer performance on small labels</td></tr>
            <tr><td><strong>Ensemble / Distilled</strong></td><td>Teacher: fine-tuned SSL; Student: compact CNN</td><td>Knowledge distillation yields small on-device model with near-teacher accuracy</td></tr>
          </tbody>
        </table>
      </div>

      <div class="feature-card">
        <h3>Feature engineering & clinical rationale</h3>
        <p>We used a hybrid approach to capture both physiologically interpretable features and learned representations:</p>
        <ul>
          <li><strong>MFCC & spectral features</strong> (centroid, bandwidth) — reflect formant and spectral energy shifts during cough/breath</li>
          <li><strong>Temporal envelope & burst energy</strong> — capture cough onset mechanics and airflow disruptions</li>
          <li><strong>Jitter / shimmer</strong> — detect voice quality changes related to inflammation</li>
          <li><strong>SSL embeddings</strong> — capture complex patterns not easily hand-designed; effective when fine-tuned with small labels</li>
        </ul>
      </div>

      <div class="feature-card">
        <h3>Data augmentation</h3>
        <p>To improve generalization and reduce overfitting we applied a curated augmentation pipeline during training:</p>
        <ul>
          <li><strong>SpecAugment:</strong> time and frequency masking on log-mel inputs</li>
          <li><strong>Environmental augmentation:</strong> additive noise (SNR schedule), RIR convolution to simulate rooms</li>
          <li><strong>Time-domain transforms:</strong> small pitch shifts, ±10% time-stretch</li>
          <li><strong>Mixup / CutMix for audio:</strong> linear combinations of same-label samples to regularize decision boundaries</li>
        </ul>
      </div>
    </section>

    <section id="results">
      <h2>Results & Impact</h2>
      
      <div class="feature-card">
        <h3>Evaluation protocol (summary)</h3>
        <ul>
          <li><strong>Patient-level splits:</strong> train / val / test with no patient overlap (default 70/15/15). External cohort reserved if available.</li>
          <li><strong>Metrics:</strong> AUROC, sensitivity & specificity at chosen thresholds, F1, and 95% bootstrap confidence intervals.</li>
          <li><strong>Reporting:</strong> per-segment and per-patient aggregated results, plus subgroup performance by age/sex/device.</li>
        </ul>
      </div>

      <div class="feature-card">
        <h3>Model benchmark (illustrative demo numbers)</h3>
        <table>
          <thead>
            <tr>
              <th>Model</th>
              <th>Params</th>
              <th>AUROC (95% CI)</th>
              <th>Sensitivity @ Spec=0.90</th>
              <th>Notes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>MFCC + XGBoost</td>
              <td>~10k</td>
              <td>0.68 (0.62–0.74)</td>
              <td>0.48</td>
              <td>Stable baseline; interpretable</td>
            </tr>
            <tr>
              <td>CNN (log-mel)</td>
              <td>~0.3M</td>
              <td>0.73 (0.68–0.78)</td>
              <td>0.55</td>
              <td>Lightweight, good target for distillation</td>
            </tr>
            <tr>
              <td>Wav2Vec2 (fine-tuned)</td>
              <td>~90M + head</td>
              <td>0.78 (0.72–0.84)</td>
              <td>0.62</td>
              <td>Strong transfer baseline</td>
            </tr>
            <tr>
              <td>Distilled compact CNN</td>
              <td>~1–3M</td>
              <td>0.74 (0.69–0.79)</td>
              <td>0.58</td>
              <td>On-device candidate after quantization</td>
            </tr>
          </tbody>
        </table>

        
      </div>

      <div class="feature-card">
        <h3>Explainability & audio biomarkers</h3>
        <p>We validated candidate biomarkers using two complementary methods:</p>
        <ul>
          <li><strong>SHAP on MFCC/tabular models:</strong> ranks the most predictive handcrafted features (example top features: spectral centroid change, spectral entropy, peak envelope energy).</li>
          <li><strong>Integrated gradients & saliency on spectrogram models:</strong> visualize frequency bands and time windows that drive positive predictions; present cropped spectrograms with overlay masks.</li>
        </ul>

        <p><strong>Example saliency figure:</strong></p>
        <img src="../assets/cough_saliency.png" alt="Spectrogram saliency example" class="hero-img">
        <p class="img-caption">Spectrogram with model saliency overlay (example). Highlighted band: <strong>500–2000 Hz</strong> (annotated as the most informative band in this demo example).</p>
      </div>

      <div class="feature-card">
        <h3>Ablations & practical findings (demo)</h3>
        <ul>
          <li><strong>Multimodal fusion:</strong> combining cough + breath + speech improved AUROC by <strong>+0.035</strong> (≈3.5 percentage points) versus the best single modality in the demo experiments.</li>
          <li><strong>Augmentation:</strong> SpecAugment + Mixup consistently improved validation AUROC by <strong>+0.02</strong> in these runs.</li>
          <li><strong>Distillation:</strong> distilling the Wav2Vec2 teacher to a compact CNN preserved ≈<strong>92%</strong> of teacher AUROC while reducing model size to ≈<strong>2.1 MB</strong> after 8-bit quantization (demo numbers).</li>
        </ul>
      </div>
      
      <div class="feature-card">
        <h3>Deployment benchmarks (example)</h3>
        <table>
          <thead>
            <tr><th>Model</th><th>Device</th><th>Size (MB)</th><th>Latency (ms)</th><th>Notes</th></tr>
          </thead>
          <tbody>
            <tr><td>Distilled CNN (8-bit)</td><td>Android mid-tier (CPU)</td><td>2.1 MB</td><td>~120 ms</td><td>On-device inference, single-shot segment</td></tr>
            <tr><td>Distilled CNN (8-bit)</td><td>iPhone mid-tier (CPU)</td><td>2.1 MB</td><td>~90 ms</td><td>On-device inference</td></tr>
          </tbody>
        </table>

        
      </div>
      
      <div class="feature-card">
        <h3>Limitations & caution</h3>
        <ul>
          
          <li>Dataset biases (device, demographic skews) can inflate reported metrics; present subgroup analyses when available.</li>
          <li>Acoustic signatures can drift with viral variants and over time — continuous monitoring and revalidation are required for production use.</li>
        </ul>
      </div>
    </section>

    <section>
      <h2>Potential Applications</h2>
      
      <div class="feature-card">
        <h3>Use cases</h3>
        <ul>
          <li><strong>Pre-screening tool:</strong> quick triage before confirmatory testing</li>
          <li><strong>Remote monitoring:</strong> track symptom progression for home-isolated patients</li>
          <li><strong>Public health surveillance:</strong> low-cost population-level monitoring</li>
          <li><strong>Clinical decision support:</strong> augment clinician workflows with non-invasive alerts</li>
        </ul>
      </div>
    </section>

    <section id="tech">
      <h2>Technology Stack & Reproducibility</h2>
      <p>Key libraries and tools used in the project (all code and notebooks are available in the repository linked below):</p>
      <div class="tech-tags">
        <span>Python</span>
        <span>PyTorch</span>
        <span>Hugging Face</span>
        <span>Librosa</span>
        <span>Torchaudio</span>
        <span>scikit-learn</span>
        <span>XGBoost</span>
        <span>SpecAugment</span>
        <span>Wav2Vec2</span>
        <span>TFLite</span>
        <span>Docker</span>
      </div>

      <div class="feature-card">
        <h3>Reproducibility checklist</h3>
        <ul>
          <li>Data README & CSV metadata (no PHI)</li>
          <li>Preprocessing scripts (segmentation, augmentation)</li>
          <li>Training scripts (configs, seeds, checkpoints)</li>
          <li>Evaluation scripts (patient-level split, bootstrap CI)</li>
          <li>Explainability scripts (SHAP, saliency)</li>
          <li>Dockerfile / requirements.txt</li>
          <li>Model card + dataset datasheet</li>
        </ul>
      </div>
    </section>

    <div class="case-footer">
      <a href="https://github.com/aragit/covid-audio" target="_blank">
        <i class="fab fa-github"></i> View Sample Code
      </a>
      <a href="../index.html#contact">
        <i class="fas fa-envelope"></i> Discuss This Project
      </a>
    </div>
  </main>
</body>
</html>
